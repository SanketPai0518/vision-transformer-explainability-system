{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 983k/170M [00:02<07:41, 367kB/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ToTensor\n\u001b[1;32m      4\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([ToTensor()])\n\u001b[0;32m----> 6\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcifar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCIFAR10\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m testset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mcifar\u001b[38;5;241m.\u001b[39mCIFAR10(\n\u001b[1;32m     14\u001b[0m     root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m     train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     16\u001b[0m     download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     17\u001b[0m     transform\u001b[38;5;241m=\u001b[39mtransform\n\u001b[1;32m     18\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torchvision/datasets/cifar.py:66\u001b[0m, in \u001b[0;36mCIFAR10.__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain \u001b[38;5;241m=\u001b[39m train  \u001b[38;5;66;03m# training set or test set\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_integrity():\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torchvision/datasets/cifar.py:139\u001b[0m, in \u001b[0;36mCIFAR10.download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_integrity():\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m \u001b[43mdownload_and_extract_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtgz_md5\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torchvision/datasets/utils.py:388\u001b[0m, in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filename:\n\u001b[1;32m    386\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(url)\n\u001b[0;32m--> 388\u001b[0m \u001b[43mdownload_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    390\u001b[0m archive \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(download_root, filename)\n\u001b[1;32m    391\u001b[0m extract_archive(archive, extract_root, remove_finished)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torchvision/datasets/utils.py:127\u001b[0m, in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# download the file\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 127\u001b[0m     \u001b[43m_urlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mURLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m url[:\u001b[38;5;241m5\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torchvision/datasets/utils.py:30\u001b[0m, in \u001b[0;36m_urlretrieve\u001b[0;34m(url, filename, chunk_size)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(url, headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: USER_AGENT})) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fh, tqdm(total\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mlength, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m---> 30\u001b[0m         \u001b[38;5;28;01mwhile\u001b[39;00m chunk \u001b[38;5;241m:=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     31\u001b[0m             fh\u001b[38;5;241m.\u001b[39mwrite(chunk)\n\u001b[1;32m     32\u001b[0m             pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/http/client.py:473\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    472\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 473\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/ssl.py:1315\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1312\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1313\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1314\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "transform = transforms.Compose([ToTensor()])\n",
    "\n",
    "train_data = datasets.cifar.CIFAR10(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "testset = datasets.cifar.CIFAR10(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader= DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader= DataLoader(testset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(images.shape)\n",
    "    print(labels.shape)\n",
    "    break   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in /opt/anaconda3/lib/python3.11/site-packages (1.0.22)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.11/site-packages (from timm) (2.9.0)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.11/site-packages (from timm) (0.24.0)\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda3/lib/python3.11/site-packages (from timm) (6.0.3)\n",
      "Requirement already satisfied: huggingface_hub in /opt/anaconda3/lib/python3.11/site-packages (from timm) (0.36.0)\n",
      "Requirement already satisfied: safetensors in /opt/anaconda3/lib/python3.11/site-packages (from timm) (0.4.3)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from huggingface_hub->timm) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface_hub->timm) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface_hub->timm) (23.2)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.11/site-packages (from huggingface_hub->timm) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface_hub->timm) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface_hub->timm) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface_hub->timm) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (2025.10.5)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.11/site-packages (from torch->timm) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/anaconda3/lib/python3.11/site-packages (from torch->timm) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from torch->timm) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->torch->timm) (3.0.3)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.11/site-packages (from torchvision->timm) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from torchvision->timm) (9.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor, Resize\n",
    "%pip install timm\n",
    "import timm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    Resize((224,224)),\n",
    "    ToTensor()\n",
    "])\n",
    "train_data = datasets.CIFAR10(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_data = datasets.CIFAR10(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "train_loader = DataLoader(train_data, batch_size=320, shuffle=True)\n",
    "test_loader  = DataLoader(test_data, batch_size=320, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model('vit_tiny_patch16_224', pretrained=True)\n",
    "model.head = nn.Linear(model.head.in_features, 10)   # CIFAR-10 has 10 classes\n",
    "model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion, device,verbose=True):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, preds = outputs.max(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        if verbose:\n",
    "            print(f\"Batch {batch_idx}: Loss={loss.item():.4f}\")\n",
    "    return total_loss / len(loader), correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, preds = outputs.max(1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    return total_loss / len(loader), correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: Loss=5.0013\n",
      "Batch 1: Loss=3.3481\n",
      "Batch 2: Loss=3.3477\n",
      "Batch 3: Loss=3.1672\n",
      "Batch 4: Loss=2.5967\n",
      "Batch 5: Loss=2.3636\n",
      "Batch 6: Loss=2.2329\n",
      "Batch 7: Loss=2.1767\n",
      "Batch 8: Loss=2.2170\n",
      "Batch 9: Loss=2.0879\n",
      "Batch 10: Loss=2.0011\n",
      "Batch 11: Loss=1.9630\n",
      "Batch 12: Loss=1.9251\n",
      "Batch 13: Loss=1.7881\n",
      "Batch 14: Loss=1.8431\n",
      "Batch 15: Loss=1.7435\n",
      "Batch 16: Loss=1.5356\n",
      "Batch 17: Loss=1.4781\n",
      "Batch 18: Loss=1.5184\n",
      "Batch 19: Loss=1.3247\n",
      "Batch 20: Loss=1.3933\n",
      "Batch 21: Loss=1.3327\n",
      "Batch 22: Loss=1.2962\n",
      "Batch 23: Loss=1.1718\n",
      "Batch 24: Loss=1.0678\n",
      "Batch 25: Loss=1.0914\n",
      "Batch 26: Loss=1.2115\n",
      "Batch 27: Loss=0.9798\n",
      "Batch 28: Loss=0.9929\n",
      "Batch 29: Loss=0.9630\n",
      "Batch 30: Loss=0.8552\n",
      "Batch 31: Loss=0.8126\n",
      "Batch 32: Loss=0.8144\n",
      "Batch 33: Loss=0.8233\n",
      "Batch 34: Loss=0.7232\n",
      "Batch 35: Loss=0.5960\n",
      "Batch 36: Loss=0.6709\n",
      "Batch 37: Loss=0.5340\n",
      "Batch 38: Loss=0.5771\n",
      "Batch 39: Loss=0.6147\n",
      "Batch 40: Loss=0.5066\n",
      "Batch 41: Loss=0.6037\n",
      "Batch 42: Loss=0.5128\n",
      "Batch 43: Loss=0.6193\n",
      "Batch 44: Loss=0.4815\n",
      "Batch 45: Loss=0.5415\n",
      "Batch 46: Loss=0.5128\n",
      "Batch 47: Loss=0.4324\n",
      "Batch 48: Loss=0.4853\n",
      "Batch 49: Loss=0.5118\n",
      "Batch 50: Loss=0.4961\n",
      "Batch 51: Loss=0.3935\n",
      "Batch 52: Loss=0.4066\n",
      "Batch 53: Loss=0.4985\n",
      "Batch 54: Loss=0.2997\n",
      "Batch 55: Loss=0.4589\n",
      "Batch 56: Loss=0.4813\n",
      "Batch 57: Loss=0.4035\n",
      "Batch 58: Loss=0.3212\n",
      "Batch 59: Loss=0.4329\n",
      "Batch 60: Loss=0.2997\n",
      "Batch 61: Loss=0.3326\n",
      "Batch 62: Loss=0.3191\n",
      "Batch 63: Loss=0.3090\n",
      "Batch 64: Loss=0.3415\n",
      "Batch 65: Loss=0.2841\n",
      "Batch 66: Loss=0.2580\n",
      "Batch 67: Loss=0.3379\n",
      "Batch 68: Loss=0.3660\n",
      "Batch 69: Loss=0.2485\n",
      "Batch 70: Loss=0.3595\n",
      "Batch 71: Loss=0.2674\n",
      "Batch 72: Loss=0.2940\n",
      "Batch 73: Loss=0.3859\n",
      "Batch 74: Loss=0.2607\n",
      "Batch 75: Loss=0.2121\n",
      "Batch 76: Loss=0.3122\n",
      "Batch 77: Loss=0.2470\n",
      "Batch 78: Loss=0.2362\n",
      "Batch 79: Loss=0.3146\n",
      "Batch 80: Loss=0.3646\n",
      "Batch 81: Loss=0.2695\n",
      "Batch 82: Loss=0.3205\n",
      "Batch 83: Loss=0.2567\n",
      "Batch 84: Loss=0.2160\n",
      "Batch 85: Loss=0.2602\n",
      "Batch 86: Loss=0.2394\n",
      "Batch 87: Loss=0.1335\n",
      "Batch 88: Loss=0.2264\n",
      "Batch 89: Loss=0.2032\n",
      "Batch 90: Loss=0.1556\n",
      "Batch 91: Loss=0.1871\n",
      "Batch 92: Loss=0.3420\n",
      "Batch 93: Loss=0.2166\n",
      "Batch 94: Loss=0.1904\n",
      "Batch 95: Loss=0.2908\n",
      "Batch 96: Loss=0.2213\n",
      "Batch 97: Loss=0.2374\n",
      "Batch 98: Loss=0.2083\n",
      "Batch 99: Loss=0.2229\n",
      "Batch 100: Loss=0.1851\n",
      "Batch 101: Loss=0.2668\n",
      "Batch 102: Loss=0.2127\n",
      "Batch 103: Loss=0.2587\n",
      "Batch 104: Loss=0.2305\n",
      "Batch 105: Loss=0.2070\n",
      "Batch 106: Loss=0.1765\n",
      "Batch 107: Loss=0.2067\n",
      "Batch 108: Loss=0.1954\n",
      "Batch 109: Loss=0.2607\n",
      "Batch 110: Loss=0.2439\n",
      "Batch 111: Loss=0.2530\n",
      "Batch 112: Loss=0.1828\n",
      "Batch 113: Loss=0.2126\n",
      "Batch 114: Loss=0.2550\n",
      "Batch 115: Loss=0.2899\n",
      "Batch 116: Loss=0.2572\n",
      "Batch 117: Loss=0.2320\n",
      "Batch 118: Loss=0.2101\n",
      "Batch 119: Loss=0.2423\n",
      "Batch 120: Loss=0.2171\n",
      "Batch 121: Loss=0.1609\n",
      "Batch 122: Loss=0.2288\n",
      "Batch 123: Loss=0.1795\n",
      "Batch 124: Loss=0.2780\n",
      "Batch 125: Loss=0.2300\n",
      "Batch 126: Loss=0.2081\n",
      "Batch 127: Loss=0.1644\n",
      "Batch 128: Loss=0.2443\n",
      "Batch 129: Loss=0.1860\n",
      "Batch 130: Loss=0.1438\n",
      "Batch 131: Loss=0.2572\n",
      "Batch 132: Loss=0.2078\n",
      "Batch 133: Loss=0.1982\n",
      "Batch 134: Loss=0.1543\n",
      "Batch 135: Loss=0.2030\n",
      "Batch 136: Loss=0.1562\n",
      "Batch 137: Loss=0.1629\n",
      "Batch 138: Loss=0.1889\n",
      "Batch 139: Loss=0.1633\n",
      "Batch 140: Loss=0.1630\n",
      "Batch 141: Loss=0.1518\n",
      "Batch 142: Loss=0.2181\n",
      "Batch 143: Loss=0.2215\n",
      "Batch 144: Loss=0.1828\n",
      "Batch 145: Loss=0.2071\n",
      "Batch 146: Loss=0.2205\n",
      "Batch 147: Loss=0.1803\n",
      "Batch 148: Loss=0.1658\n",
      "Batch 149: Loss=0.1500\n",
      "Batch 150: Loss=0.2240\n",
      "Batch 151: Loss=0.1390\n",
      "Batch 152: Loss=0.2038\n",
      "Batch 153: Loss=0.2133\n",
      "Batch 154: Loss=0.1474\n",
      "Batch 155: Loss=0.1960\n",
      "Batch 156: Loss=0.2026\n",
      "Epoch 1/5\n",
      "Train Loss: 0.6116 | Train Acc: 0.7974\n",
      "Val Loss:   0.1695 | Val Acc:   0.9408\n",
      "Batch 0: Loss=0.1161\n",
      "Batch 1: Loss=0.1029\n",
      "Batch 2: Loss=0.1160\n",
      "Batch 3: Loss=0.0895\n",
      "Batch 4: Loss=0.1199\n",
      "Batch 5: Loss=0.0603\n",
      "Batch 6: Loss=0.1268\n",
      "Batch 7: Loss=0.1847\n",
      "Batch 8: Loss=0.1263\n",
      "Batch 9: Loss=0.1730\n",
      "Batch 10: Loss=0.0912\n",
      "Batch 11: Loss=0.0692\n",
      "Batch 12: Loss=0.1882\n",
      "Batch 13: Loss=0.1018\n",
      "Batch 14: Loss=0.1496\n",
      "Batch 15: Loss=0.0758\n",
      "Batch 16: Loss=0.2281\n",
      "Batch 17: Loss=0.1782\n",
      "Batch 18: Loss=0.1831\n",
      "Batch 19: Loss=0.1701\n",
      "Batch 20: Loss=0.1247\n",
      "Batch 21: Loss=0.1094\n",
      "Batch 22: Loss=0.1216\n",
      "Batch 23: Loss=0.0853\n",
      "Batch 24: Loss=0.0952\n",
      "Batch 25: Loss=0.1442\n",
      "Batch 26: Loss=0.1011\n",
      "Batch 27: Loss=0.0989\n",
      "Batch 28: Loss=0.1557\n",
      "Batch 29: Loss=0.1084\n",
      "Batch 30: Loss=0.1551\n",
      "Batch 31: Loss=0.1093\n",
      "Batch 32: Loss=0.1519\n",
      "Batch 33: Loss=0.1166\n",
      "Batch 34: Loss=0.1036\n",
      "Batch 35: Loss=0.1619\n",
      "Batch 36: Loss=0.1338\n",
      "Batch 37: Loss=0.1551\n",
      "Batch 38: Loss=0.0991\n",
      "Batch 39: Loss=0.1110\n",
      "Batch 40: Loss=0.1462\n",
      "Batch 41: Loss=0.1040\n",
      "Batch 42: Loss=0.1351\n",
      "Batch 43: Loss=0.1197\n",
      "Batch 44: Loss=0.1629\n",
      "Batch 45: Loss=0.0865\n",
      "Batch 46: Loss=0.1830\n",
      "Batch 47: Loss=0.1204\n",
      "Batch 48: Loss=0.1139\n",
      "Batch 49: Loss=0.1944\n",
      "Batch 50: Loss=0.0792\n",
      "Batch 51: Loss=0.1304\n",
      "Batch 52: Loss=0.1426\n",
      "Batch 53: Loss=0.0939\n",
      "Batch 54: Loss=0.1045\n",
      "Batch 55: Loss=0.1299\n",
      "Batch 56: Loss=0.1193\n",
      "Batch 57: Loss=0.1163\n",
      "Batch 58: Loss=0.0735\n",
      "Batch 59: Loss=0.1764\n",
      "Batch 60: Loss=0.1424\n",
      "Batch 61: Loss=0.1233\n",
      "Batch 62: Loss=0.1397\n",
      "Batch 63: Loss=0.0998\n",
      "Batch 64: Loss=0.1970\n",
      "Batch 65: Loss=0.1285\n",
      "Batch 66: Loss=0.1476\n",
      "Batch 67: Loss=0.1427\n",
      "Batch 68: Loss=0.1357\n",
      "Batch 69: Loss=0.1357\n",
      "Batch 70: Loss=0.1707\n",
      "Batch 71: Loss=0.0752\n",
      "Batch 72: Loss=0.1438\n",
      "Batch 73: Loss=0.1351\n",
      "Batch 74: Loss=0.0837\n",
      "Batch 75: Loss=0.1441\n",
      "Batch 76: Loss=0.1672\n",
      "Batch 77: Loss=0.1402\n",
      "Batch 78: Loss=0.1156\n",
      "Batch 79: Loss=0.1417\n",
      "Batch 80: Loss=0.1018\n",
      "Batch 81: Loss=0.1033\n",
      "Batch 82: Loss=0.1806\n",
      "Batch 83: Loss=0.1535\n",
      "Batch 84: Loss=0.1063\n",
      "Batch 85: Loss=0.1324\n",
      "Batch 86: Loss=0.1949\n",
      "Batch 87: Loss=0.1471\n",
      "Batch 88: Loss=0.1729\n",
      "Batch 89: Loss=0.1463\n",
      "Batch 90: Loss=0.1379\n",
      "Batch 91: Loss=0.1809\n",
      "Batch 92: Loss=0.1527\n",
      "Batch 93: Loss=0.1082\n",
      "Batch 94: Loss=0.1911\n",
      "Batch 95: Loss=0.1452\n",
      "Batch 96: Loss=0.1981\n",
      "Batch 97: Loss=0.1092\n",
      "Batch 98: Loss=0.1669\n",
      "Batch 99: Loss=0.1254\n",
      "Batch 100: Loss=0.1324\n",
      "Batch 101: Loss=0.2380\n",
      "Batch 102: Loss=0.1247\n",
      "Batch 103: Loss=0.1021\n",
      "Batch 104: Loss=0.1061\n",
      "Batch 105: Loss=0.1177\n",
      "Batch 106: Loss=0.1409\n",
      "Batch 107: Loss=0.1587\n",
      "Batch 108: Loss=0.0833\n",
      "Batch 109: Loss=0.1341\n",
      "Batch 110: Loss=0.1507\n",
      "Batch 111: Loss=0.1400\n",
      "Batch 112: Loss=0.1244\n",
      "Batch 113: Loss=0.1292\n",
      "Batch 114: Loss=0.1308\n",
      "Batch 115: Loss=0.1325\n",
      "Batch 116: Loss=0.1303\n",
      "Batch 117: Loss=0.1377\n",
      "Batch 118: Loss=0.1883\n",
      "Batch 119: Loss=0.0908\n",
      "Batch 120: Loss=0.1459\n",
      "Batch 121: Loss=0.1641\n",
      "Batch 122: Loss=0.1221\n",
      "Batch 123: Loss=0.0871\n",
      "Batch 124: Loss=0.0986\n",
      "Batch 125: Loss=0.1225\n",
      "Batch 126: Loss=0.1075\n",
      "Batch 127: Loss=0.1071\n",
      "Batch 128: Loss=0.0919\n",
      "Batch 129: Loss=0.0965\n",
      "Batch 130: Loss=0.2069\n",
      "Batch 131: Loss=0.1345\n",
      "Batch 132: Loss=0.0948\n",
      "Batch 133: Loss=0.2257\n",
      "Batch 134: Loss=0.1166\n",
      "Batch 135: Loss=0.0985\n",
      "Batch 136: Loss=0.2038\n",
      "Batch 137: Loss=0.1526\n",
      "Batch 138: Loss=0.1077\n",
      "Batch 139: Loss=0.1363\n",
      "Batch 140: Loss=0.1461\n",
      "Batch 141: Loss=0.1666\n",
      "Batch 142: Loss=0.1379\n",
      "Batch 143: Loss=0.0905\n",
      "Batch 144: Loss=0.1209\n",
      "Batch 145: Loss=0.1499\n",
      "Batch 146: Loss=0.1364\n",
      "Batch 147: Loss=0.1762\n",
      "Batch 148: Loss=0.0862\n",
      "Batch 149: Loss=0.2041\n",
      "Batch 150: Loss=0.1480\n",
      "Batch 151: Loss=0.0945\n",
      "Batch 152: Loss=0.1230\n",
      "Batch 153: Loss=0.1435\n",
      "Batch 154: Loss=0.1054\n",
      "Batch 155: Loss=0.1347\n",
      "Batch 156: Loss=0.0474\n",
      "Epoch 2/5\n",
      "Train Loss: 0.1326 | Train Acc: 0.9534\n",
      "Val Loss:   0.1510 | Val Acc:   0.9489\n",
      "Batch 0: Loss=0.0917\n",
      "Batch 1: Loss=0.0590\n",
      "Batch 2: Loss=0.0599\n",
      "Batch 3: Loss=0.0431\n",
      "Batch 4: Loss=0.0863\n",
      "Batch 5: Loss=0.0410\n",
      "Batch 6: Loss=0.0473\n",
      "Batch 7: Loss=0.0635\n",
      "Batch 8: Loss=0.0499\n",
      "Batch 9: Loss=0.0822\n",
      "Batch 10: Loss=0.0826\n",
      "Batch 11: Loss=0.1185\n",
      "Batch 12: Loss=0.0630\n",
      "Batch 13: Loss=0.0541\n",
      "Batch 14: Loss=0.0398\n",
      "Batch 15: Loss=0.0612\n",
      "Batch 16: Loss=0.0896\n",
      "Batch 17: Loss=0.0479\n",
      "Batch 18: Loss=0.0258\n",
      "Batch 19: Loss=0.0734\n",
      "Batch 20: Loss=0.0902\n",
      "Batch 21: Loss=0.0507\n",
      "Batch 22: Loss=0.0380\n",
      "Batch 23: Loss=0.0531\n",
      "Batch 24: Loss=0.0545\n",
      "Batch 25: Loss=0.0595\n",
      "Batch 26: Loss=0.0546\n",
      "Batch 27: Loss=0.0546\n",
      "Batch 28: Loss=0.0286\n",
      "Batch 29: Loss=0.0250\n",
      "Batch 30: Loss=0.0541\n",
      "Batch 31: Loss=0.0422\n",
      "Batch 32: Loss=0.0383\n",
      "Batch 33: Loss=0.0562\n",
      "Batch 34: Loss=0.0424\n",
      "Batch 35: Loss=0.0599\n",
      "Batch 36: Loss=0.0340\n",
      "Batch 37: Loss=0.0365\n",
      "Batch 38: Loss=0.0935\n",
      "Batch 39: Loss=0.0534\n",
      "Batch 40: Loss=0.0635\n",
      "Batch 41: Loss=0.0468\n",
      "Batch 42: Loss=0.0824\n",
      "Batch 43: Loss=0.0982\n",
      "Batch 44: Loss=0.0641\n",
      "Batch 45: Loss=0.0647\n",
      "Batch 46: Loss=0.0381\n",
      "Batch 47: Loss=0.1132\n",
      "Batch 48: Loss=0.0493\n",
      "Batch 49: Loss=0.0652\n",
      "Batch 50: Loss=0.0344\n",
      "Batch 51: Loss=0.0459\n",
      "Batch 52: Loss=0.0970\n",
      "Batch 53: Loss=0.1203\n",
      "Batch 54: Loss=0.0907\n",
      "Batch 55: Loss=0.1530\n",
      "Batch 56: Loss=0.0277\n",
      "Batch 57: Loss=0.0724\n",
      "Batch 58: Loss=0.1221\n",
      "Batch 59: Loss=0.0719\n",
      "Batch 60: Loss=0.0863\n",
      "Batch 61: Loss=0.1184\n",
      "Batch 62: Loss=0.0712\n",
      "Batch 63: Loss=0.0842\n",
      "Batch 64: Loss=0.0483\n",
      "Batch 65: Loss=0.0517\n",
      "Batch 66: Loss=0.0832\n",
      "Batch 67: Loss=0.0882\n",
      "Batch 68: Loss=0.1062\n",
      "Batch 69: Loss=0.1132\n",
      "Batch 70: Loss=0.0873\n",
      "Batch 71: Loss=0.0863\n",
      "Batch 72: Loss=0.1531\n",
      "Batch 73: Loss=0.0716\n",
      "Batch 74: Loss=0.0657\n",
      "Batch 75: Loss=0.1109\n",
      "Batch 76: Loss=0.0752\n",
      "Batch 77: Loss=0.0624\n",
      "Batch 78: Loss=0.0943\n",
      "Batch 79: Loss=0.0694\n",
      "Batch 80: Loss=0.0529\n",
      "Batch 81: Loss=0.0489\n",
      "Batch 82: Loss=0.0921\n",
      "Batch 83: Loss=0.1111\n",
      "Batch 84: Loss=0.1443\n",
      "Batch 85: Loss=0.0799\n",
      "Batch 86: Loss=0.1059\n",
      "Batch 87: Loss=0.0963\n",
      "Batch 88: Loss=0.0747\n",
      "Batch 89: Loss=0.0691\n",
      "Batch 90: Loss=0.0487\n",
      "Batch 91: Loss=0.0791\n",
      "Batch 92: Loss=0.0619\n",
      "Batch 93: Loss=0.0942\n",
      "Batch 94: Loss=0.1166\n",
      "Batch 95: Loss=0.0689\n",
      "Batch 96: Loss=0.0390\n",
      "Batch 97: Loss=0.0445\n",
      "Batch 98: Loss=0.0405\n",
      "Batch 99: Loss=0.0969\n",
      "Batch 100: Loss=0.0539\n",
      "Batch 101: Loss=0.0773\n",
      "Batch 102: Loss=0.0830\n",
      "Batch 103: Loss=0.1236\n",
      "Batch 104: Loss=0.1143\n",
      "Batch 105: Loss=0.0726\n",
      "Batch 106: Loss=0.0588\n",
      "Batch 107: Loss=0.0933\n",
      "Batch 108: Loss=0.1108\n",
      "Batch 109: Loss=0.1398\n",
      "Batch 110: Loss=0.1101\n",
      "Batch 111: Loss=0.0918\n",
      "Batch 112: Loss=0.1161\n",
      "Batch 113: Loss=0.0557\n",
      "Batch 114: Loss=0.0437\n",
      "Batch 115: Loss=0.0585\n",
      "Batch 116: Loss=0.0898\n",
      "Batch 117: Loss=0.1071\n",
      "Batch 118: Loss=0.0552\n",
      "Batch 119: Loss=0.0527\n",
      "Batch 120: Loss=0.1264\n",
      "Batch 121: Loss=0.0952\n",
      "Batch 122: Loss=0.0822\n",
      "Batch 123: Loss=0.1287\n",
      "Batch 124: Loss=0.0783\n",
      "Batch 125: Loss=0.0927\n",
      "Batch 126: Loss=0.0716\n",
      "Batch 127: Loss=0.1003\n",
      "Batch 128: Loss=0.1087\n",
      "Batch 129: Loss=0.0439\n",
      "Batch 130: Loss=0.0822\n",
      "Batch 131: Loss=0.1279\n",
      "Batch 132: Loss=0.0860\n",
      "Batch 133: Loss=0.0625\n",
      "Batch 134: Loss=0.0759\n",
      "Batch 135: Loss=0.1067\n",
      "Batch 136: Loss=0.0743\n",
      "Batch 137: Loss=0.0617\n",
      "Batch 138: Loss=0.0631\n",
      "Batch 139: Loss=0.0981\n",
      "Batch 140: Loss=0.0659\n",
      "Batch 141: Loss=0.0859\n",
      "Batch 142: Loss=0.0899\n",
      "Batch 143: Loss=0.0738\n",
      "Batch 144: Loss=0.0901\n",
      "Batch 145: Loss=0.0835\n",
      "Batch 146: Loss=0.0678\n",
      "Batch 147: Loss=0.1036\n",
      "Batch 148: Loss=0.0933\n",
      "Batch 149: Loss=0.0557\n",
      "Batch 150: Loss=0.1082\n",
      "Batch 151: Loss=0.0706\n",
      "Batch 152: Loss=0.0911\n",
      "Batch 153: Loss=0.0459\n",
      "Batch 154: Loss=0.0893\n",
      "Batch 155: Loss=0.0752\n",
      "Batch 156: Loss=0.0069\n",
      "Epoch 3/5\n",
      "Train Loss: 0.0757 | Train Acc: 0.9730\n",
      "Val Loss:   0.1568 | Val Acc:   0.9494\n",
      "Batch 0: Loss=0.0433\n",
      "Batch 1: Loss=0.0719\n",
      "Batch 2: Loss=0.0639\n",
      "Batch 3: Loss=0.0358\n",
      "Batch 4: Loss=0.0326\n",
      "Batch 5: Loss=0.0381\n",
      "Batch 6: Loss=0.0278\n",
      "Batch 7: Loss=0.1120\n",
      "Batch 8: Loss=0.0581\n",
      "Batch 9: Loss=0.0602\n",
      "Batch 10: Loss=0.0630\n",
      "Batch 11: Loss=0.0497\n",
      "Batch 12: Loss=0.0306\n",
      "Batch 13: Loss=0.0763\n",
      "Batch 14: Loss=0.0424\n",
      "Batch 15: Loss=0.0275\n",
      "Batch 16: Loss=0.0825\n",
      "Batch 17: Loss=0.0345\n",
      "Batch 18: Loss=0.0441\n",
      "Batch 19: Loss=0.0298\n",
      "Batch 20: Loss=0.0409\n",
      "Batch 21: Loss=0.0543\n",
      "Batch 22: Loss=0.0503\n",
      "Batch 23: Loss=0.0400\n",
      "Batch 24: Loss=0.0329\n",
      "Batch 25: Loss=0.0471\n",
      "Batch 26: Loss=0.0409\n",
      "Batch 27: Loss=0.0517\n",
      "Batch 28: Loss=0.0162\n",
      "Batch 29: Loss=0.0463\n",
      "Batch 30: Loss=0.0410\n",
      "Batch 31: Loss=0.0342\n",
      "Batch 32: Loss=0.0354\n",
      "Batch 33: Loss=0.0359\n",
      "Batch 34: Loss=0.0429\n",
      "Batch 35: Loss=0.0359\n",
      "Batch 36: Loss=0.0253\n",
      "Batch 37: Loss=0.0342\n",
      "Batch 38: Loss=0.0577\n",
      "Batch 39: Loss=0.0224\n",
      "Batch 40: Loss=0.0420\n",
      "Batch 41: Loss=0.0489\n",
      "Batch 42: Loss=0.0320\n",
      "Batch 43: Loss=0.0467\n",
      "Batch 44: Loss=0.0267\n",
      "Batch 45: Loss=0.0241\n",
      "Batch 46: Loss=0.0261\n",
      "Batch 47: Loss=0.0389\n",
      "Batch 48: Loss=0.0402\n",
      "Batch 49: Loss=0.0607\n",
      "Batch 50: Loss=0.0237\n",
      "Batch 51: Loss=0.0512\n",
      "Batch 52: Loss=0.0452\n",
      "Batch 53: Loss=0.0657\n",
      "Batch 54: Loss=0.0751\n",
      "Batch 55: Loss=0.0683\n",
      "Batch 56: Loss=0.0640\n",
      "Batch 57: Loss=0.0627\n",
      "Batch 58: Loss=0.0387\n",
      "Batch 59: Loss=0.1250\n",
      "Batch 60: Loss=0.0470\n",
      "Batch 61: Loss=0.0342\n",
      "Batch 62: Loss=0.0953\n",
      "Batch 63: Loss=0.0682\n",
      "Batch 64: Loss=0.0809\n",
      "Batch 65: Loss=0.0356\n",
      "Batch 66: Loss=0.0594\n",
      "Batch 67: Loss=0.0822\n",
      "Batch 68: Loss=0.0460\n",
      "Batch 69: Loss=0.0644\n",
      "Batch 70: Loss=0.0363\n",
      "Batch 71: Loss=0.0412\n",
      "Batch 72: Loss=0.0456\n",
      "Batch 73: Loss=0.0579\n",
      "Batch 74: Loss=0.0722\n",
      "Batch 75: Loss=0.0481\n",
      "Batch 76: Loss=0.0724\n",
      "Batch 77: Loss=0.0918\n",
      "Batch 78: Loss=0.0364\n",
      "Batch 79: Loss=0.0483\n",
      "Batch 80: Loss=0.0524\n",
      "Batch 81: Loss=0.0511\n",
      "Batch 82: Loss=0.0490\n",
      "Batch 83: Loss=0.0889\n",
      "Batch 84: Loss=0.0644\n",
      "Batch 85: Loss=0.0608\n",
      "Batch 86: Loss=0.0432\n",
      "Batch 87: Loss=0.0540\n",
      "Batch 88: Loss=0.0678\n",
      "Batch 89: Loss=0.0346\n",
      "Batch 90: Loss=0.0331\n",
      "Batch 91: Loss=0.0791\n",
      "Batch 92: Loss=0.0505\n",
      "Batch 93: Loss=0.0753\n",
      "Batch 94: Loss=0.0410\n",
      "Batch 95: Loss=0.0564\n",
      "Batch 96: Loss=0.0353\n",
      "Batch 97: Loss=0.0647\n",
      "Batch 98: Loss=0.0990\n",
      "Batch 99: Loss=0.0970\n",
      "Batch 100: Loss=0.0607\n",
      "Batch 101: Loss=0.0805\n",
      "Batch 102: Loss=0.0459\n",
      "Batch 103: Loss=0.0612\n",
      "Batch 104: Loss=0.0772\n",
      "Batch 105: Loss=0.0666\n",
      "Batch 106: Loss=0.0683\n",
      "Batch 107: Loss=0.0675\n",
      "Batch 108: Loss=0.0876\n",
      "Batch 109: Loss=0.0927\n",
      "Batch 110: Loss=0.0795\n",
      "Batch 111: Loss=0.0389\n",
      "Batch 112: Loss=0.0403\n",
      "Batch 113: Loss=0.0838\n",
      "Batch 114: Loss=0.0491\n",
      "Batch 115: Loss=0.0622\n",
      "Batch 116: Loss=0.0755\n",
      "Batch 117: Loss=0.0352\n",
      "Batch 118: Loss=0.0915\n",
      "Batch 119: Loss=0.0467\n",
      "Batch 120: Loss=0.0748\n",
      "Batch 121: Loss=0.0514\n",
      "Batch 122: Loss=0.0907\n",
      "Batch 123: Loss=0.0302\n",
      "Batch 124: Loss=0.0618\n",
      "Batch 125: Loss=0.0886\n",
      "Batch 126: Loss=0.1107\n",
      "Batch 127: Loss=0.0997\n",
      "Batch 128: Loss=0.0740\n",
      "Batch 129: Loss=0.0804\n",
      "Batch 130: Loss=0.0716\n",
      "Batch 131: Loss=0.1146\n",
      "Batch 132: Loss=0.1077\n",
      "Batch 133: Loss=0.0512\n",
      "Batch 134: Loss=0.1404\n",
      "Batch 135: Loss=0.1218\n",
      "Batch 136: Loss=0.0621\n",
      "Batch 137: Loss=0.0665\n",
      "Batch 138: Loss=0.0283\n",
      "Batch 139: Loss=0.0804\n",
      "Batch 140: Loss=0.0735\n",
      "Batch 141: Loss=0.0773\n",
      "Batch 142: Loss=0.0772\n",
      "Batch 143: Loss=0.0514\n",
      "Batch 144: Loss=0.0611\n",
      "Batch 145: Loss=0.0593\n",
      "Batch 146: Loss=0.0529\n",
      "Batch 147: Loss=0.0724\n",
      "Batch 148: Loss=0.0288\n",
      "Batch 149: Loss=0.0563\n",
      "Batch 150: Loss=0.0671\n",
      "Batch 151: Loss=0.0787\n",
      "Batch 152: Loss=0.0315\n",
      "Batch 153: Loss=0.0707\n",
      "Batch 154: Loss=0.0851\n",
      "Batch 155: Loss=0.0557\n",
      "Batch 156: Loss=0.0508\n",
      "Epoch 4/5\n",
      "Train Loss: 0.0582 | Train Acc: 0.9796\n",
      "Val Loss:   0.1890 | Val Acc:   0.9456\n",
      "Batch 0: Loss=0.0433\n",
      "Batch 1: Loss=0.0555\n",
      "Batch 2: Loss=0.0423\n",
      "Batch 3: Loss=0.0165\n",
      "Batch 4: Loss=0.0420\n",
      "Batch 5: Loss=0.0451\n",
      "Batch 6: Loss=0.0340\n",
      "Batch 7: Loss=0.0336\n",
      "Batch 8: Loss=0.0600\n",
      "Batch 9: Loss=0.0484\n",
      "Batch 10: Loss=0.0143\n",
      "Batch 11: Loss=0.0407\n",
      "Batch 12: Loss=0.0579\n",
      "Batch 13: Loss=0.0214\n",
      "Batch 14: Loss=0.0561\n",
      "Batch 15: Loss=0.0933\n",
      "Batch 16: Loss=0.0230\n",
      "Batch 17: Loss=0.0481\n",
      "Batch 18: Loss=0.0673\n",
      "Batch 19: Loss=0.0537\n",
      "Batch 20: Loss=0.0247\n",
      "Batch 21: Loss=0.0633\n",
      "Batch 22: Loss=0.0623\n",
      "Batch 23: Loss=0.0296\n",
      "Batch 24: Loss=0.0342\n",
      "Batch 25: Loss=0.0359\n",
      "Batch 26: Loss=0.0566\n",
      "Batch 27: Loss=0.0372\n",
      "Batch 28: Loss=0.0833\n",
      "Batch 29: Loss=0.0573\n",
      "Batch 30: Loss=0.0342\n",
      "Batch 31: Loss=0.0323\n",
      "Batch 32: Loss=0.0598\n",
      "Batch 33: Loss=0.0445\n",
      "Batch 34: Loss=0.0340\n",
      "Batch 35: Loss=0.0476\n",
      "Batch 36: Loss=0.0591\n",
      "Batch 37: Loss=0.0540\n",
      "Batch 38: Loss=0.0489\n",
      "Batch 39: Loss=0.0688\n",
      "Batch 40: Loss=0.0558\n",
      "Batch 41: Loss=0.0301\n",
      "Batch 42: Loss=0.0445\n",
      "Batch 43: Loss=0.0645\n",
      "Batch 44: Loss=0.0268\n",
      "Batch 45: Loss=0.0258\n",
      "Batch 46: Loss=0.0318\n",
      "Batch 47: Loss=0.0876\n",
      "Batch 48: Loss=0.0638\n",
      "Batch 49: Loss=0.0689\n",
      "Batch 50: Loss=0.0737\n",
      "Batch 51: Loss=0.0481\n",
      "Batch 52: Loss=0.0422\n",
      "Batch 53: Loss=0.0759\n",
      "Batch 54: Loss=0.0728\n",
      "Batch 55: Loss=0.0610\n",
      "Batch 56: Loss=0.0243\n",
      "Batch 57: Loss=0.0930\n",
      "Batch 58: Loss=0.0376\n",
      "Batch 59: Loss=0.0356\n",
      "Batch 60: Loss=0.0370\n",
      "Batch 61: Loss=0.0572\n",
      "Batch 62: Loss=0.0288\n",
      "Batch 63: Loss=0.0594\n",
      "Batch 64: Loss=0.0367\n",
      "Batch 65: Loss=0.0240\n",
      "Batch 66: Loss=0.0470\n",
      "Batch 67: Loss=0.0356\n",
      "Batch 68: Loss=0.0698\n",
      "Batch 69: Loss=0.0716\n",
      "Batch 70: Loss=0.0205\n",
      "Batch 71: Loss=0.0269\n",
      "Batch 72: Loss=0.1153\n",
      "Batch 73: Loss=0.0449\n",
      "Batch 74: Loss=0.0681\n",
      "Batch 75: Loss=0.0628\n",
      "Batch 76: Loss=0.0381\n",
      "Batch 77: Loss=0.0518\n",
      "Batch 78: Loss=0.0486\n",
      "Batch 79: Loss=0.0716\n",
      "Batch 80: Loss=0.0333\n",
      "Batch 81: Loss=0.0214\n",
      "Batch 82: Loss=0.0644\n",
      "Batch 83: Loss=0.0500\n",
      "Batch 84: Loss=0.0462\n",
      "Batch 85: Loss=0.0580\n",
      "Batch 86: Loss=0.0239\n",
      "Batch 87: Loss=0.0369\n",
      "Batch 88: Loss=0.0553\n",
      "Batch 89: Loss=0.0394\n",
      "Batch 90: Loss=0.0402\n",
      "Batch 91: Loss=0.0133\n",
      "Batch 92: Loss=0.0412\n",
      "Batch 93: Loss=0.0264\n",
      "Batch 94: Loss=0.0187\n",
      "Batch 95: Loss=0.0401\n",
      "Batch 96: Loss=0.0826\n",
      "Batch 97: Loss=0.0648\n",
      "Batch 98: Loss=0.0404\n",
      "Batch 99: Loss=0.0189\n",
      "Batch 100: Loss=0.0296\n",
      "Batch 101: Loss=0.0572\n",
      "Batch 102: Loss=0.0308\n",
      "Batch 103: Loss=0.0286\n",
      "Batch 104: Loss=0.0422\n",
      "Batch 105: Loss=0.0335\n",
      "Batch 106: Loss=0.0457\n",
      "Batch 107: Loss=0.0185\n",
      "Batch 108: Loss=0.0633\n",
      "Batch 109: Loss=0.0702\n",
      "Batch 110: Loss=0.0429\n",
      "Batch 111: Loss=0.0626\n",
      "Batch 112: Loss=0.0615\n",
      "Batch 113: Loss=0.0483\n",
      "Batch 114: Loss=0.0437\n",
      "Batch 115: Loss=0.0533\n",
      "Batch 116: Loss=0.0332\n",
      "Batch 117: Loss=0.0375\n",
      "Batch 118: Loss=0.0200\n",
      "Batch 119: Loss=0.0292\n",
      "Batch 120: Loss=0.0388\n",
      "Batch 121: Loss=0.0271\n",
      "Batch 122: Loss=0.0533\n",
      "Batch 123: Loss=0.0334\n",
      "Batch 124: Loss=0.0661\n",
      "Batch 125: Loss=0.0203\n",
      "Batch 126: Loss=0.0602\n",
      "Batch 127: Loss=0.0734\n",
      "Batch 128: Loss=0.0485\n",
      "Batch 129: Loss=0.0859\n",
      "Batch 130: Loss=0.0171\n",
      "Batch 131: Loss=0.0681\n",
      "Batch 132: Loss=0.0661\n",
      "Batch 133: Loss=0.0811\n",
      "Batch 134: Loss=0.0455\n",
      "Batch 135: Loss=0.0536\n",
      "Batch 136: Loss=0.0684\n",
      "Batch 137: Loss=0.0507\n",
      "Batch 138: Loss=0.0597\n",
      "Batch 139: Loss=0.0682\n",
      "Batch 140: Loss=0.0219\n",
      "Batch 141: Loss=0.0634\n",
      "Batch 142: Loss=0.0456\n",
      "Batch 143: Loss=0.0457\n",
      "Batch 144: Loss=0.0444\n",
      "Batch 145: Loss=0.0282\n",
      "Batch 146: Loss=0.0736\n",
      "Batch 147: Loss=0.0555\n",
      "Batch 148: Loss=0.0325\n",
      "Batch 149: Loss=0.0336\n",
      "Batch 150: Loss=0.0880\n",
      "Batch 151: Loss=0.0223\n",
      "Batch 152: Loss=0.0432\n",
      "Batch 153: Loss=0.0516\n",
      "Batch 154: Loss=0.0901\n",
      "Batch 155: Loss=0.0754\n",
      "Batch 156: Loss=0.1582\n",
      "Epoch 5/5\n",
      "Train Loss: 0.0488 | Train Acc: 0.9825\n",
      "Val Loss:   0.2450 | Val Acc:   0.9319\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./models/vit_cifar10.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"./models/vit_cifar10.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
